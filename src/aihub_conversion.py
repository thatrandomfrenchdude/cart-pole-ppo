import torch
import torch.onnx
import yaml
import os
import sys
import qai_hub as hub

# Add parent directory to Python path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.network import PPONetwork  # Your model class

# Create a deployment-friendly wrapper
class PPODeploymentNetwork(torch.nn.Module):
    def __init__(self, ppo_network):
        super(PPODeploymentNetwork, self).__init__()
        self.ppo_network = ppo_network
    
    def forward(self, x):
        action_probs, _ = self.ppo_network(x)
        return action_probs

# ‚úÖ Load Config
def load_config(path='config.yaml'):
    if os.path.exists(path):
        with open(path, 'r') as f:
            return yaml.safe_load(f)
    else:
        return {'network': {'input_dim': 4, 'hidden_dim': 128, 'output_dim': 2}}

# ‚úÖ Quantize Model
def quantize_model(pth_path, config):
    # Load the checkpoint
    checkpoint = torch.load(pth_path, map_location='cpu')
    
    # Load the original PPO network
    ppo_model = PPONetwork(config)
    ppo_model.load_state_dict(checkpoint['network_state_dict'])
    ppo_model.eval()
    
    # Create deployment wrapper (only action predictions)
    model_fp32 = PPODeploymentNetwork(ppo_model)
    model_fp32.eval()

    # For simplicity, let's skip quantization for now as it can be complex with custom models
    # Instead, we'll use the FP32 model directly
    example_input = torch.rand(1, config['network']['input_dim'])
    
    # Test the model works
    with torch.no_grad():
        output = model_fp32(example_input)
        print(f"Model output shape: {output.shape}")
        print(f"Sample prediction: {output}")

    return model_fp32, example_input

# ‚úÖ Export to ONNX
def export_onnx(model, example_input, path='model_deployment.onnx'):
    """Export PyTorch model to ONNX format for onnxruntime-qnn"""
    try:
        torch.onnx.export(
            model,
            example_input,
            path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        print(f"Saved ONNX model to: {path}")
        return path
    except Exception as e:
        print(f"‚ùå Error exporting to ONNX: {e}")
        return None

# ‚úÖ Export to TorchScript
def export_torchscript(model, example_input, path='model_deployment.pt'):
    traced = torch.jit.trace(model, example_input)
    torch.jit.save(traced, path)
    print(f"Saved TorchScript model to: {path}")
    return path

# ‚úÖ Compile for Snapdragon X Elite
def compile_model(pt_path, input_dim, save_compiled=True):
    device = hub.Device("Snapdragon X Elite CRD")
    compile_job = hub.submit_compile_job(
        model=pt_path,
        device=device,
        options="--target_runtime qnn_context_binary",
        input_specs={"input": (1, input_dim)}
    )
    compile_job.wait()
    print("‚úîÔ∏è Compilation finished")
    
    compiled_model = compile_job.get_target_model()
    
    # Save the compiled model locally
    if save_compiled:
        try:
            compiled_model_path = "model_compiled_qnn.bin"
            with open(compiled_model_path, 'wb') as f:
                f.write(compiled_model.read())
            print(f"üíæ Saved compiled QNN model to: {compiled_model_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not save compiled model locally: {e}")
            print("The compiled model is available through the AI Hub job")
    
    return compiled_model

# ‚úÖ Profile the Model
def profile_model(compiled_model):
    device = hub.Device("Snapdragon X Elite CRD")
    profile_job = hub.submit_profile_job(model=compiled_model, device=device)
    profile_job.wait()
    print("üìä Profile Results:")
    try:
        # Try different methods to get results
        if hasattr(profile_job, 'get_profile'):
            results = profile_job.get_profile()
        elif hasattr(profile_job, 'get_profile_data'):
            results = profile_job.get_profile_data()
        else:
            results = "Profile completed successfully but results format may have changed"
        print(results)
    except Exception as e:
        print(f"Profile completed but couldn't retrieve detailed results: {e}")
        print("Check the AI Hub dashboard for detailed profiling results.")

# ‚úÖ Main Pipeline
if __name__ == "__main__":
    print("üöÄ Starting AI Hub conversion pipeline...")
    
    # Load configuration
    config = load_config()
    print(f"‚úÖ Config loaded: {config['network']}")
    
    # Load and prepare model
    print("üì¶ Loading and preparing model...")
    model_fp32, example_input = quantize_model("example/model.pth", config)
    
    # Export to TorchScript
    print("üìÑ Exporting to TorchScript...")
    pt_path = export_torchscript(model_fp32, example_input)
    
    # Export to ONNX (for onnxruntime-qnn)
    print("üìÑ Exporting to ONNX...")
    onnx_path = export_onnx(model_fp32, example_input)
    
    # Compile for Snapdragon X Elite
    print("üîß Compiling for Snapdragon X Elite...")
    try:
        compiled_model = compile_model(pt_path, config['network']['input_dim'])
        
        # Profile the model
        print("üìä Profiling model performance...")
        profile_model(compiled_model)
        
        print("‚úÖ Pipeline completed successfully!")
        print("\nüìã Generated Files:")
        print(f"  ‚Ä¢ TorchScript: {pt_path}")
        if onnx_path:
            print(f"  ‚Ä¢ ONNX: {onnx_path}")
        print(f"  ‚Ä¢ Compiled QNN: model_compiled_qnn.bin (if saved)")
        
    except Exception as e:
        print(f"‚ùå Error during AI Hub compilation: {e}")
        print("This might require valid AI Hub credentials and device access.")
        print(f"‚úÖ TorchScript model was successfully created at: {pt_path}")
        if onnx_path:
            print(f"‚úÖ ONNX model was successfully created at: {onnx_path}")